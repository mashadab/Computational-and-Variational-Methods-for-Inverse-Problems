{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca57f69",
   "metadata": {},
   "source": [
    "# Computational and Variational Methods for Inverse Problems\n",
    "\n",
    "## Assignment 2: Optimization methods and automatic differentiation\n",
    "\n",
    "### (Notebook outline for problems 2, 3, and 4)\n",
    "\n",
    "### Your name: Mohammad Afzal Shadab\n",
    "(type your name here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ceebb",
   "metadata": {},
   "source": [
    "## Dependencies:\n",
    "\n",
    " - numpy\n",
    " - scipy\n",
    " - matplotlib\n",
    " - autograd (https://github.com/HIPS/autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad, jacobian\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69709ac9",
   "metadata": {},
   "source": [
    "# 2) Discretized length and energy functionals\n",
    "\n",
    "In this problem you will write code to compute the length functional $L(u)$ and energy functional $E(u)$ from problem 1, for continuous piecewise linear functions $u:[0,1]\\rightarrow \\mathbb{R}$ with equally spaced nodes. We assume that $u$ has $N+2$ equally spaced nodes at locations\n",
    "$$0, h, 2h, 3h, \\dots, Nh, 1$$\n",
    "and that $u$ takes specified values at the nodes, and is piecewise linear in the intervals (\"cells\") between these nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69e4d0",
   "metadata": {},
   "source": [
    "## 2a) Spatial derivative of piecewise linear function\n",
    "Let $u(x)$ be a continuous piecewise linear function on $[0,1]$, with $N+2$ equally spaced nodes (including endpoints). The spatial derivative $\\frac{du}{dx}$ is piecewise constant on the $N+1$ cells between neighboring nodes. An example of this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02094d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = np.linspace(0.0, 1.0, 5)\n",
    "u_at_nodes = np.array([0.0, 1.0, -0.5, 0.25, 0.0])\n",
    "\n",
    "cell_centers = 0.5 * (nodes[1:] + nodes[:-1])\n",
    "du_dx_in_cells = np.array([ 4., -6.,  3., -1.])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(nodes, u_at_nodes)\n",
    "plt.plot(nodes, u_at_nodes, '.r')\n",
    "plt.title('u')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(nodes, np.concatenate([du_dx_in_cells, [-1]]), drawstyle='steps-post')\n",
    "plt.plot(cell_centers, du_dx_in_cells, '*g')\n",
    "plt.title('du/dx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0bdf6",
   "metadata": {},
   "source": [
    "### Your task:\n",
    "Write a function called \"spatial\\_derivative\" that takes the length-$(N+2)$ vector of nodal values for $u$ as input, and returns the length-$N+1$ vector of cell values of $\\frac{du}{dx}$ as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aed023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_derivative(u_at_nodes):\n",
    "    ####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####\n",
    "    return du_dx_in_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a4b38",
   "metadata": {},
   "source": [
    "### Test:\n",
    "Error should be close to machine epsilon (around $10^{-14}$ or smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([0.25, -0.66,  0.70, -0.41, -1.02, 0.57,  0.13, -1.27,  0.83, -1.21])\n",
    "du_dx_true = np.array([-8.19,  12.24,  -9.99,  -5.49,  14.31,  -3.96, -12.6,  18.9, -18.36])\n",
    "\n",
    "du_dx = spatial_derivative(u)\n",
    "spatial_derivative_error = np.linalg.norm(du_dx - du_dx_true)\n",
    "print('spatial_derivative_error=', spatial_derivative_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afb2b0",
   "metadata": {},
   "source": [
    "## 2b) Arc length function\n",
    "\n",
    "In Problem 1 you showed that the arc length of a function $u:[0,1]\\rightarrow \\mathbb{R}$ is given by\n",
    "$$L(u) = \\int_0^1 \\sqrt{1 + \\left(\\frac{du}{dx}(x)\\right)^2}  dx$$\n",
    "\n",
    "### Your task:\n",
    "Let $u$ be a continuous piecewise linear function on $[0,1]$, with $N+2$ equally spaced nodes, including endpoints (the same setup as in 2a). Write a function called \"length\\_functional\" that takes the vector of $N+2$ nodal values of $u$ as input, and returns the arc length of $u$ as output.\n",
    "\n",
    "#### Hint:\n",
    "The integrand is a piecewise constant function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_functional(u_at_nodes):\n",
    "    ####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54c66b",
   "metadata": {},
   "source": [
    "### Test\n",
    "Error should be close to machine epsilon (around $10^{-14}$ or smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0.0, 1.0, 100)\n",
    "u = np.sqrt(1.0 / 2.0 - np.power(xx - 0.5, 2))\n",
    "L_true = 1.1107087105088733\n",
    "\n",
    "L = length_functional(u)\n",
    "length_functional_error = np.abs(L - L_true)\n",
    "print('length_functional_error=', length_functional_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d261e73",
   "metadata": {},
   "source": [
    "## 2c) Energy functional\n",
    "\n",
    "Let $u$ be the height of an idealized hanging chain, and for convenience let the linear density of the chain be $\\rho=1/g$, where the $g$ is the gravitational constant. From Problem 1 we know that the potential energy functional for such a hanging chain is given by\n",
    "$$E(u) = \\int_0^1 u(x) \\sqrt{1 + \\left(\\frac{du}{dx}(x)\\right)^2}  dx.$$\n",
    "\n",
    "### Your task\n",
    "Write a function named \"energy\\_functional\" that takes as input the length $N+2$ vector of nodal values for a continuous piecewise linear function $u$ on $[0,1]$ as input (including values at the endpoints, as in 2a and 2b), and returns the energy functional $E(u)$ as output.\n",
    "\n",
    "#### Hint:\n",
    "The integrand is a **discontinuous** piecewise linear function because it is the product of a continuous piecewise linear function with a piecewise constant function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_functional(u_at_nodes):\n",
    "    ####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbadf743",
   "metadata": {},
   "source": [
    "### Test\n",
    "Error should be close to machine epsilon (around $10^{-14}$ or smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db97e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0.0, 1.0, 75)\n",
    "u = -np.sin(np.pi * xx)\n",
    "E_true = -1.2379425688273515\n",
    "\n",
    "E = energy_functional(u)\n",
    "energy_functional_error = np.abs(E - E_true)\n",
    "print('energy_functional_error=', energy_functional_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1dabb2",
   "metadata": {},
   "source": [
    "# 3)  Objective function, Gradient, and Hessian\n",
    "\n",
    "In problem 1 we saw that the hanging chain problem may be written as the following constrained optimization problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\min_u &\\quad E(u) \\\\\n",
    "\\text{such that} &\\quad L(u) = L_0.\n",
    "\\end{align}\n",
    "\n",
    "We may find approximate solutions to this problem by solving the following unconstrained optimization problem,\n",
    "$$\\min_u E(u) + \\alpha (L(u) - L_0)^2$$\n",
    "where we use a quadratic penalty to approximately enforce the constraint. Here $\\alpha > 0$ is a penalty parameter; as $\\alpha \\rightarrow \\infty$, the solution to the unconstrained optimization problem approaches the solution of the constrained optimization problem.\n",
    "\n",
    "Let\n",
    "$$J(u) := E(u) + \\alpha (L(u) - L_0)^2$$\n",
    "denote the unconstrained objective function. In this problem, you will implement this objective function for continuous piecewise linear functions with equally spaced nodes, and use **automatic differentiation** to compute the gradient of $J$, the Hessian of $J$, and matrix-vector products of the Hessian of $J$ with arbitrary vectors. You will also compare the gradient and Hessian of $J$ to approximate gradients and Hessians computed with finite differences with a variety of step sizes.\n",
    "\n",
    "## Objective function\n",
    "The objective function may be written as follows, using your energy and length functional code from the previous problem. Note that the objective function only operates on the $N$ interior nodes for $u$, since the endpoints of $u$ are fixed at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(u_at_interior_nodes, alpha, L0):\n",
    "    u_at_nodes = np.hstack([np.array([0.0]),\n",
    "                                     u_at_interior_nodes,\n",
    "                                     np.array([0.0])])\n",
    "    \n",
    "    E = energy_functional(u_at_nodes)\n",
    "    L = length_functional(u_at_nodes)\n",
    "    J = E + alpha * np.power(L - L0, 2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b3922",
   "metadata": {},
   "source": [
    "If you were unable to complete the previous problem, you may uncomment the following (intentionally obfsucated) implementation and use it instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fda939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_function(u_at_interior_vertices, alpha, L0):\n",
    "#     u = np.concatenate([np.array([0]),\n",
    "#                         u_at_interior_vertices,\n",
    "#                         np.array([0])])\n",
    "#     a = 1./(len(u)-1.)\n",
    "#     b, c = (u[1:]-u[:-1])/a, 0.5*(u[1:] + u[:-1])\n",
    "#     return a*np.sum(c*np.sqrt(1.+np.power(b,2)))+alpha*np.power(a*np.sum(np.sqrt(1.+np.power(b,2)))-L0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41540008",
   "metadata": {},
   "source": [
    "## 3a) Gradient via automatic differentiation\n",
    "\n",
    "Here we will use the \"grad\" function in the python automatic differentiation tool **autograd** to compute the gradient of your objective function. Here is a quick example of how to use it. More details can be found here:\n",
    "https://github.com/HIPS/autograd/blob/master/docs/tutorial.md\n",
    "\n",
    "### autodiff grad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b):\n",
    "    return np.cos(a * x[0]) + np.sin(b * x[1])\n",
    "\n",
    "grad_autodiff = grad(f, 0) # 0 indicates differentiation w.r.t. the first argument of f.\n",
    "\n",
    "x = np.random.randn(2)\n",
    "a = 1.3\n",
    "b = -2.1\n",
    "grad_at_x_autodiff = grad_autodiff(x, a, b)\n",
    "\n",
    "grad_at_x_analytic = np.array([- a * np.sin(a * x[0]), \n",
    "                                 b * np.cos(b * x[1])])\n",
    "\n",
    "autodiff_error = np.linalg.norm(grad_at_x_analytic - grad_at_x_autodiff)\n",
    "print('autodiff_error=', autodiff_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140367e",
   "metadata": {},
   "source": [
    "### Your task:\n",
    "Use automatic differentiation to create a function named \"compute\\_gradient\" that computes the gradient of the objective function, $g$, at a given $u$ for a given penalty parameter $\\alpha$ and chain length $L_0$.\n",
    "\n",
    "### Hint:\n",
    "The code is a one-liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the function compute_gradient(u_at_interior_nodes, alpha, L0)\n",
    "\n",
    "####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af317937",
   "metadata": {},
   "source": [
    "### Finite difference check:\n",
    "On one hand, the directional derivative of $J$ in direction $p$ may be computed from the gradient at $u$, $g(u)$, as follows:\n",
    "$$\\frac{d J}{d u}(u) p = g(u)^T p.$$\n",
    "On the other hand, we may approximate the directional derivative via finite differences\n",
    "$$\\frac{d J}{du}(u) p \\approx \\frac{J(u + s p) - J(u)}{s}$$\n",
    "where $s$ is some small scalar.\n",
    "\n",
    "This allows us to check our gradient by choosing a random direction $p$, and comparing the result of computing $\\frac{d J}{d u}(u) p$ in these two different ways. \n",
    "\n",
    "The error should be roughly the same order of magnitude as the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5637fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 53\n",
    "alpha = 8.95\n",
    "L0 = 2.13\n",
    "u1 = np.random.randn(N)\n",
    "J1 = objective_function(u1, alpha, L0)\n",
    "\n",
    "p = np.random.randn(N)\n",
    "s = 1e-7 # step size\n",
    "u2 = u1 + s*p\n",
    "\n",
    "J2 = objective_function(u2, alpha, L0)\n",
    "dJ_du_p_diff = (J2 - J1) / s\n",
    "\n",
    "g = compute_gradient(u1, alpha, L0)\n",
    "dJ_du_p = np.dot(g, p)\n",
    "\n",
    "grad_err = np.abs( (dJ_du_p - dJ_du_p_diff) / dJ_du_p_diff )\n",
    "print('step size=', s, ', gradient finite difference error=', grad_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6ca9d",
   "metadata": {},
   "source": [
    "## 3b) Gradient finite difference plot\n",
    "\n",
    "It is good practice to check your gradient with finite difference checks for a wide range of step sizes $s$, and make a log-log plot of the finite difference error as a function of $s$. If your gradient is working correctly, the resulting plot should look like a 'V', where the error decreases linearly from $s=10^0$ until somewhere around $s=10^{-5}$ to $s=10^{-9}$, then goes up again and/or becomes jagged.\n",
    "\n",
    "### Example finite difference plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(x[0]) + np.cos(x[1])\n",
    "\n",
    "def df_dx(x):\n",
    "    return np.array([np.cos(x[0]), -np.sin(x[1])])\n",
    "\n",
    "x1 = np.random.randn(2)\n",
    "p = np.random.randn(2)\n",
    "step_sizes = [1e0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13]\n",
    "\n",
    "f1 = f(x1)\n",
    "g1 = df_dx(x1)\n",
    "df_dx_p = np.dot(g1, p)\n",
    "\n",
    "grad_errs = list()\n",
    "for s in step_sizes:\n",
    "    x2 = x1 + s*p\n",
    "\n",
    "    f2 = f(x2)\n",
    "    df_dx_p_diff = (f2 - f1) / s\n",
    "\n",
    "    grad_err = np.abs( (df_dx_p - df_dx_p_diff) / df_dx_p_diff )\n",
    "    print('step size=', s, ', example gradient finite difference error=', grad_err)\n",
    "    \n",
    "    grad_errs.append(grad_err)\n",
    "\n",
    "plt.loglog(step_sizes, grad_errs)\n",
    "plt.title('Example finite difference gradient check')\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24170c48",
   "metadata": {},
   "source": [
    "### Your task:\n",
    "Perform finite difference checks on the hanging chain gradient for a wide range  of step sizes $s$, and make a log-log plot of the finite difference error as a function of $s$. See 3a for an example of a finite difference check for a single step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9051ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 53\n",
    "alpha = 8.95\n",
    "L0 = 2.13\n",
    "u1 = np.random.randn(N)\n",
    "p = np.random.randn(N)\n",
    "step_sizes = [1e0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13]\n",
    "\n",
    "\n",
    "# Make log-log plot of gradient finite difference error vs. step size\n",
    "\n",
    "####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad554e95",
   "metadata": {},
   "source": [
    "## 3c) Hessian-vector product via automatic differentiation\n",
    "\n",
    "The Hessian is the derivative of the gradient, so the matrix-vector multiplication $H(u)p$ of the Hessian at $u$, $H(u)$, with a vector, $p$ is given by\n",
    "$$H(u)p = \\frac{d}{du} \\left(\\frac{d J}{du}(u) p\\right) = \\frac{d}{du} \\left(g(u)^T p\\right).$$\n",
    "In other words, $H(u) p$ is the gradient of the scalar-valued function\n",
    "$$q(u) := g(u)^T p,$$\n",
    "and this gradient may be calculated by automatic differentiation.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b):\n",
    "    return np.cos(a * x[0]) + np.sin(b * x[1])\n",
    "\n",
    "compute_grad_f = grad(f, 0)\n",
    "\n",
    "def q(x, p, a, b):\n",
    "    g = compute_grad_f(x, a, b)\n",
    "    return np.dot(g, p)\n",
    "\n",
    "hessian_vector_product_f = grad(q, 0)\n",
    "\n",
    "a = 1.4\n",
    "b = 4.1\n",
    "x = np.random.randn(2)\n",
    "p = np.random.randn(2)\n",
    "\n",
    "Hp_autodiff = hessian_vector_product_f(x, p, a, b)\n",
    "\n",
    "H_analytic = np.array([[-a*a*np.cos(a * x[0]),                     0],\n",
    "                       [0,                     -b*b*np.sin(b * x[1])]])\n",
    "\n",
    "Hp_analytic = np.dot(H_analytic, p)\n",
    "\n",
    "hess_prod_err = np.linalg.norm(Hp_analytic - Hp_autodiff) / np.linalg.norm(Hp_analytic)\n",
    "print('example hessian vector product error=', hess_prod_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42215352",
   "metadata": {},
   "source": [
    "### Your task:\n",
    "Use autodiff's grad function to create a function named \"compute\\_hessian\\_vector\\_product\" that computes the Hessian-vector product $H(u)p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ad955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the function compute_hessian_vector_product(u_at_interior_nodes, p_at_interior_nodes, alpha, L0)\n",
    "\n",
    "####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7834a",
   "metadata": {},
   "source": [
    "### Finite difference check\n",
    "The error should be roughly the same order of magnitude as the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16774e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 53\n",
    "alpha = 8.95\n",
    "L0 = 2.13\n",
    "u1 = np.random.randn(N)\n",
    "G1 = compute_gradient(u1, alpha, L0)\n",
    "\n",
    "p = np.random.randn(N)\n",
    "s = 1e-7\n",
    "u2 = u1 + s * p\n",
    "G2 = compute_gradient(u2, alpha, L0)\n",
    "\n",
    "Hp_diff = (G2 - G1) / s\n",
    "\n",
    "Hp = compute_hessian_vector_product(u1, p, alpha, L0)\n",
    "\n",
    "hessian_vector_product_error = np.linalg.norm(Hp - Hp_diff) / np.linalg.norm(Hp_diff)\n",
    "\n",
    "print('s=', s, ', hessian vector product finite difference error=', hessian_vector_product_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e76a9",
   "metadata": {},
   "source": [
    "## 3d) Hessian-vector product finite difference check plot\n",
    "\n",
    "### Your task:\n",
    "Construct a log-log plot of the Hessian finite difference error for a wide range of step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 53\n",
    "alpha = 8.95\n",
    "L0 = 2.13\n",
    "u1 = np.random.randn(N)\n",
    "p = np.random.randn(N)\n",
    "step_sizes = [1e0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13]\n",
    "\n",
    "\n",
    "# Make log-log plot of gradient finite difference error vs. step size\n",
    "\n",
    "####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3168a",
   "metadata": {},
   "source": [
    "### Comparison with dense Hessian\n",
    "\n",
    "The  $N \\times N$ Hessian matrix is the Jacobian of the gradient, which can also be computed via automatic differentiation. This is not recommended for large problems.\n",
    "\n",
    "Here we show how to compute the  Hessian using the Jacobian function in autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0422ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_hessian = jacobian(compute_gradient, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049a2cb",
   "metadata": {},
   "source": [
    "### Test dense Hessian\n",
    "Error should be of the order machine epsilon (less than $10^{-14}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "alpha = 8.95\n",
    "L0 = 2.13\n",
    "u1 = np.random.randn(N)\n",
    "\n",
    "H = compute_hessian(u1, alpha, L0)\n",
    "\n",
    "p = np.random.randn(N)\n",
    "Hp_dense = np.dot(H, p)\n",
    "Hp = compute_hessian_vector_product(u1, p, alpha, L0)\n",
    "\n",
    "dense_hessian_error = np.linalg.norm(Hp_dense - Hp) / np.linalg.norm(Hp)\n",
    "print('dense_hessian_error=', dense_hessian_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6550eb9",
   "metadata": {},
   "source": [
    "# 4) Comparison of optimization methods\n",
    "\n",
    "In this problem, we will minimize $J$ using the method of steepest descent, BFGS, and the Newton-Conjugate-Gradient method. We will also plot convergence curves, and estimate the rate of convergence for these methods.\n",
    "\n",
    "We will show you how to do this using BFGS as an example, and you will do this for steepest descent and Newton-CG.\n",
    "\n",
    "### Problem parameters\n",
    "\n",
    "For this problem, please use $N=32$, $L_0 = 3.0$, and $\\alpha=1e2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22adaf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "L0 = 3.0\n",
    "alpha = 1e2\n",
    "\n",
    "# Useful versions of the objective, gradient, \n",
    "# and hessian-vector product functions\n",
    "J_func = lambda u: objective_function(u, alpha, L0)\n",
    "g_func = lambda u: compute_gradient(u, alpha, L0)\n",
    "Hp_func = lambda u, p: compute_hessian_vector_product(u, p, alpha, L0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247017e8",
   "metadata": {},
   "source": [
    "### Initial guess\n",
    "\n",
    "A reasonable initial guess is the parabola\n",
    "$$u_0(x) = -C x(1-x)$$\n",
    "with constant \n",
    "$$C = 2 \\left(L_0 - 1\\right).$$\n",
    "The constant $C$ is chosen to make the arc length of $u$ close to $L_0$. The formula for $C$ can be derived by the sum of the lengths of the left, right, and bottom edges of the box that bounds the parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_including_endpoints = np.linspace(0.0, 1.0, N+2)\n",
    "\n",
    "C = -2.0 * (L0 - 1.0)\n",
    "u0_including_endpoints = C * xx_including_endpoints * (1.0-xx_including_endpoints)\n",
    "u0 = u0_including_endpoints[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d87257",
   "metadata": {},
   "source": [
    "### Example: BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43715d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u0.copy()\n",
    "JJ_bfgs = list()\n",
    "def callback(uk):\n",
    "    JJ_bfgs.append(J_func(uk))\n",
    "    \n",
    "sol = optimize.minimize(J_func, u0, method='BFGS', jac=g_func, callback=callback, tol=1e-6)\n",
    "print(sol)\n",
    "    \n",
    "u = sol.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cc8fe",
   "metadata": {},
   "source": [
    "### BFGS: check that solution gradient is small and plot the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_g0 = np.linalg.norm(g_func(u0))\n",
    "norm_g = np.linalg.norm(g_func(u))\n",
    "print('||g(u)|| / ||g(u_0)||=', norm_g / norm_g0) # should be less than 1e-6\n",
    "\n",
    "u_including_endpoints = np.concatenate([[0], u, [0]])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(u_including_endpoints)\n",
    "plt.title('u (BFGS)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62505f",
   "metadata": {},
   "source": [
    "### BFGS: Make a convergence plot and estimate convergence rate\n",
    "\n",
    "Asymptotically, the error should decrease as\n",
    "$$J(u_{k+1}) - J(u_*) \\le c \\left(J(u_k) - J(u_*)\\right)^q$$\n",
    "for some constant $c$ and convergence rate $q$, where $u_*$ is the solution to the optimization problem. \n",
    "\n",
    "We can estimate the convergence rate, $q$, by plotting $J(u_{k+1}) - J(u_*)$ vs. $J(u_k) - J(u_*)$ on a log-log plot and finding the slope. We use the final value of $J$ as the solution $J(u_*)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = np.logspace(-12, 0, 13)\n",
    "yy10 = np.power(kk, 1.0)\n",
    "yy15 = np.power(kk, 1.5)\n",
    "yy20 = np.power(kk, 2.0)\n",
    "plt.loglog(kk, yy10, 'r')\n",
    "plt.loglog(kk, yy15, 'g')\n",
    "plt.loglog(kk, yy20, 'b')\n",
    "\n",
    "plt.loglog(JJ_bfgs[:-2] - JJ_bfgs[-1], JJ_bfgs[1:-1] - JJ_bfgs[-1], 'k')\n",
    "plt.title('Convergence plot BFGS')\n",
    "plt.ylabel('J(u_{k+1}) - J(u_*)')\n",
    "plt.xlabel('J(u_{k}) - J(u_*)')\n",
    "plt.legend(['q=1.0', 'q=1.5', 'q=2.0', 'BFGS convergence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec8793",
   "metadata": {},
   "source": [
    "## 4a) Steepest descent\n",
    "\n",
    "The method of steepest descent minimizes the function $J(u)$ via the iteration\n",
    "$$u_{k+1} = u_k - \\beta\n",
    "_kg(u_k)$$\n",
    "where $u_k$ is the $k\\text{th}$ iterate, $g(u_k)$ is the gradient at the $k\\text{th}$ iterate, and $\\beta_k$ is a step size parameter.\n",
    "\n",
    "### Your task:\n",
    "\n",
    "Minimize $J$ using the method of steepest descent. You should write your own steepest descent code for this (do not use existing steepest descent libraries).\n",
    "\n",
    "Continue iterating until $||g_k|| < 10^{-6} ||g_0||$ or smaller. This should require several thousand iterations.\n",
    "\n",
    "Save the function values $J(u_k)$ for all iterations so we can plot the convergence.\n",
    "\n",
    "\n",
    "#### Hint: step size\n",
    "\n",
    "The trick here is choosing $\\beta_k$. If $\\beta_k$ is too large, the method may become unstable and diverge. If $\\beta_k$ is too small, the method will converge slowly. \n",
    "\n",
    "A popular option is to choose a step length satisfying the Wolfe conditions:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Wolfe_conditions\n",
    "\n",
    "You may use the function scipy.optimize.line_search to find a step length satisfying these conditions:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html#scipy.optimize.line_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b7c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u0.copy() # update u each iteration\n",
    "\n",
    "J0 = J_func(u)\n",
    "g0 = g_func(u)\n",
    "norm_g0 = np.linalg.norm(g0)\n",
    "\n",
    "JJ_sd = list()\n",
    "JJ_sd.append(J0) # Keep appending the value of J to this list each iteration\n",
    "\n",
    "\n",
    "# Solve optimization problem using method of steepest descent. \n",
    "\n",
    "####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####\n",
    "    \n",
    "# Now the solution should be in the variable u\n",
    "# and JJ_sd should be a list, with JJ[k] = J(u_k)\n",
    "\n",
    "JJ_sd = np.array(JJ_sd) # Make JJ_sd into array instead of list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1f87b",
   "metadata": {},
   "source": [
    "### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_g = np.linalg.norm(g_func(u))\n",
    "print('||g(u)|| / ||g(u_0)||=', norm_g / norm_g0) # should be less than 1e-6\n",
    "\n",
    "u_including_endpoints = np.concatenate([[0], u, [0]])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(u_including_endpoints)\n",
    "plt.title('u (steepest descent)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fdd7c",
   "metadata": {},
   "source": [
    "## 4b) Steepest descent convergence\n",
    "\n",
    "Asymptotically, the error should decrease as\n",
    "$$J(u_{k+1}) - J(u_*) \\le c \\left(J(u_k) - J(u_*)\\right)^q$$\n",
    "for some constant $c$ and convergence rate $q$, where $u_*$ is the solution to the optimization problem. \n",
    "\n",
    "### Your task:\n",
    "\n",
    "Estimate the convergence rate, $q$, by plotting $J(u_{k+1}) - J(u_*)$ vs. $J(u_k) - J(u_*)$ on a log-log plot and finding the slope. You may use the final value of $J$ as the solution $J(u_*)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a log-log convergence plot for steepest descent and estimate the convergence rate\n",
    "\n",
    "####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3499e5e",
   "metadata": {},
   "source": [
    "## 4c) Newton-CG\n",
    "\n",
    "### Your task:\n",
    "\n",
    "Minimize $J$ using the Newton-CG method implemented in scipy.optimize.minimize:\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html\n",
    "\n",
    " - Use a tolerance of $10^{-6}$\n",
    "\n",
    " - Save the intermediate values of $J(u_k)$ so that we can study convergence.\n",
    "\n",
    "\n",
    "### Hint:\n",
    "To save intermediate values of $J(u_k)$, use a \"callback\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u0.copy()\n",
    "JJ_ncg = list()\n",
    "def callback(uk):\n",
    "    JJ_ncg.append(J_func(uk))\n",
    "\n",
    "sol = optimize.minimize(####    YOUR CODE HERE    ####)\n",
    "print(sol)\n",
    "    \n",
    "u = sol.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd1cee",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6296fa5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_g = np.linalg.norm(g_func(u))\n",
    "print('||g(u)|| / ||g(u_0)||=', norm_g / norm_g0) # should be less than 1e-6\n",
    "\n",
    "u_including_endpoints = np.concatenate([[0], u, [0]])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(u_including_endpoints)\n",
    "plt.title('u (Newton-CG)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4752bb3",
   "metadata": {},
   "source": [
    "## 4d) Newton-CG convergence\n",
    "\n",
    "Asymptotically, the error should decrease as\n",
    "$$J(u_{k+1}) - J(u_*) \\le c \\left(J(u_k) - J(u_*)\\right)^q$$\n",
    "for some constant $c$ and convergence rate $q$, where $u_*$ is the solution to the optimization problem. \n",
    "\n",
    "### Your task:\n",
    "\n",
    "Estimate the convergence rate, $q$, for Newton-CG by plotting $J(u_{k+1}) - J(u_*)$ vs. $J(u_k) - J(u_*)$ on a log-log plot and estimating the slope. You may use the final value of $J$ as the solution $J(u_*)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a log-log convergence plot for Newton-CG and estimate the convergence rate\n",
    "\n",
    "####    VVVV   YOUR CODE GOES HERE    VVVV    ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####    ^^^^   YOUR CODE GOES HERE    ^^^^    ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
